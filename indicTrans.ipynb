{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52b8885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import uvicorn\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad7a2a",
   "metadata": {},
   "source": [
    "#### Optional transliteration fallback (no indictrans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14885e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from indic_transliteration import sanscript\n",
    "    from indic_transliteration.sanscript import transliterate as itransliterate\n",
    "except Exception:\n",
    "    sanscript = None\n",
    "    itransliterate = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4045ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_ID = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "INDIC_EN_MODEL_ID = \"ai4bharat/indictrans2-indic-en-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf7e0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_TAGS = {\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"hi\": \"hin_Deva\",\n",
    "    \"ta\": \"tam_Taml\",\n",
    "    \"te\": \"tel_Telu\",\n",
    "    \"kn\": \"kan_Knda\",\n",
    "    \"ml\": \"mal_Mlym\",\n",
    "    \"bn\": \"ben_Beng\",\n",
    "    \"mr\": \"mar_Deva\",\n",
    "    \"gu\": \"guj_Gujr\",\n",
    "    \"or\": \"ory_Orya\",  # if this fails for your model snapshot, try \"ori_Orya\"\n",
    "    \"pa\": \"pan_Guru\"\n",
    "}\n",
    "\n",
    "LANGUAGE_ISO3 = {k: v.split(\"_\")[0] for k, v in LANGUAGE_TAGS.items()}\n",
    "LANGUAGE_SCRIPT = {k: v.split(\"_\")[1] for k, v in LANGUAGE_TAGS.items()}\n",
    "\n",
    "# Unicode script ranges for sanity check\n",
    "SCRIPT_RANGES = {\n",
    "    \"Latn\": (0x0041, 0x007A),  # coarse Latin range (A-z)\n",
    "    \"Deva\": (0x0900, 0x097F),\n",
    "    \"Beng\": (0x0980, 0x09FF),\n",
    "    \"Guru\": (0x0A00, 0x0A7F),\n",
    "    \"Gujr\": (0x0A80, 0x0AFF),\n",
    "    \"Orya\": (0x0B00, 0x0B7F),\n",
    "    \"Taml\": (0x0B80, 0x0BFF),\n",
    "    \"Telu\": (0x0C00, 0x0C7F),\n",
    "    \"Knda\": (0x0C80, 0x0CFF),\n",
    "    \"Mlym\": (0x0D00, 0x0D7F),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30437a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_script(s: str, script: str) -> bool:\n",
    "    lo, hi = SCRIPT_RANGES.get(script, (None, None))\n",
    "    if lo is None:  # unknown script key → don't block\n",
    "        return True\n",
    "    return any(lo <= ord(ch) <= hi for ch in s)\n",
    "\n",
    "# Map our script keys -> indic-transliteration constants\n",
    "SANSCRIPT_MAP = None\n",
    "if sanscript is not None:\n",
    "    SANSCRIPT_MAP = {\n",
    "        \"Deva\": getattr(sanscript, \"DEVANAGARI\", None),\n",
    "        \"Beng\": getattr(sanscript, \"BENGALI\", None),\n",
    "        \"Guru\": getattr(sanscript, \"GURMUKHI\", None),\n",
    "        \"Gujr\": getattr(sanscript, \"GUJARATI\", None),\n",
    "        \"Orya\": getattr(sanscript, \"ORIYA\", None),   # a.k.a. Odia\n",
    "        \"Taml\": getattr(sanscript, \"TAMIL\", None),\n",
    "        \"Telu\": getattr(sanscript, \"TELUGU\", None),\n",
    "        \"Knda\": getattr(sanscript, \"KANNADA\", None),\n",
    "        \"Mlym\": getattr(sanscript, \"MALAYALAM\", None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a89829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_if_needed(text: str, target_script: str) -> str:\n",
    "    \"\"\"\n",
    "    If output isn't in target script but is Devanagari, try converting\n",
    "    Devanagari -> target_script using indic-transliteration (if available).\n",
    "    \"\"\"\n",
    "    if looks_like_script(text, target_script):\n",
    "        return text\n",
    "    if looks_like_script(text, \"Deva\") and itransliterate and SANSCRIPT_MAP:\n",
    "        src = SANSCRIPT_MAP.get(\"Deva\")\n",
    "        dst = SANSCRIPT_MAP.get(target_script)\n",
    "        if src and dst:\n",
    "            try:\n",
    "                return itransliterate(text, src, dst)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b375e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# App lifecycle\n",
    "# -----------------------------\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    try:\n",
    "        print(\"Starting up... Loading model/tokenizer/pipelines\")\n",
    "        get_tokenizer()\n",
    "        get_model()\n",
    "        get_translation_pipe_en_to_indic()\n",
    "        get_translation_pipe_indic_to_en()\n",
    "        print(\"Resources loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not pre-load resources: {e}\")\n",
    "    yield\n",
    "    print(\"Shutting down...\")\n",
    "\n",
    "app = FastAPI(lifespan=lifespan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a09c2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static & templates\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "templates = Jinja2Templates(directory=\"templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aee1bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORS (relax in dev)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5df3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cached loaders\n",
    "# -----------------------------\n",
    "@lru_cache(maxsize=1)\n",
    "def load_tokenizer():\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None\n",
    "    )\n",
    "    print(\"Tokenizer loaded\")\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b2218fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "    configs = [\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None,\n",
    "             low_cpu_mem_usage=True),\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None),\n",
    "        dict(trust_remote_code=True),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for i, cfg in enumerate(configs, 1):\n",
    "        try:\n",
    "            print(f\"Trying model strategy {i}...\")\n",
    "            mdl = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, **cfg)\n",
    "\n",
    "            # Disable caching to avoid \"Cache only has 0 layers\"\n",
    "            for attr in (\"config\", \"generation_config\"):\n",
    "                obj = getattr(mdl, attr, None)\n",
    "                if obj is not None:\n",
    "                    try: setattr(obj, \"use_cache\", False)\n",
    "                    except Exception: pass\n",
    "            try: mdl.config.cache_implementation = None\n",
    "            except Exception: pass\n",
    "            try: mdl.cache_implementation = None\n",
    "            except Exception: pass\n",
    "\n",
    "            mdl.eval().to(DEVICE)\n",
    "            print(f\"Model loaded on {DEVICE}\")\n",
    "            return mdl\n",
    "        except Exception as e:\n",
    "            print(f\"Strategy {i} failed: {e}\")\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed to load model: {last_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb74beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def get_tokenizer():\n",
    "    global tokenizer\n",
    "    if tokenizer is None:\n",
    "        tokenizer = load_tokenizer()\n",
    "    return tokenizer\n",
    "\n",
    "def get_model():\n",
    "    global model\n",
    "    if model is None:\n",
    "        model = load_model()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2234f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional loaders for Indic→English\n",
    "@lru_cache(maxsize=1)\n",
    "def load_tokenizer_indic_en():\n",
    "    print(\"Loading INDIC→EN tokenizer...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        INDIC_EN_MODEL_ID,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None\n",
    "    )\n",
    "    print(\"INDIC→EN tokenizer loaded\")\n",
    "    return tok\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_model_indic_en():\n",
    "    print(\"Loading INDIC→EN model...\")\n",
    "    configs = [\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None,\n",
    "             low_cpu_mem_usage=True),\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None),\n",
    "        dict(trust_remote_code=True),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for i, cfg in enumerate(configs, 1):\n",
    "        try:\n",
    "            print(f\"Trying INDIC→EN model strategy {i}...\")\n",
    "            mdl = AutoModelForSeq2SeqLM.from_pretrained(INDIC_EN_MODEL_ID, **cfg)\n",
    "            for attr in (\"config\", \"generation_config\"):\n",
    "                obj = getattr(mdl, attr, None)\n",
    "                if obj is not None:\n",
    "                    try: setattr(obj, \"use_cache\", False)\n",
    "                    except Exception: pass\n",
    "            try: mdl.config.cache_implementation = None\n",
    "            except Exception: pass\n",
    "            try: mdl.cache_implementation = None\n",
    "            except Exception: pass\n",
    "            mdl.eval().to(DEVICE)\n",
    "            print(f\"INDIC→EN model loaded on {DEVICE}\")\n",
    "            return mdl\n",
    "        except Exception as e:\n",
    "            print(f\"INDIC→EN strategy {i} failed: {e}\")\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed to load INDIC→EN model: {last_err}\")\n",
    "\n",
    "# Getters\n",
    "@lru_cache(maxsize=1)\n",
    "def get_tokenizer_indic_en():\n",
    "    return load_tokenizer_indic_en()\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_model_indic_en():\n",
    "    return load_model_indic_en()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dd65bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def get_translation_pipe_en_to_indic():\n",
    "    tok = get_tokenizer()\n",
    "    mdl = get_model()\n",
    "    device_idx = 0 if torch.cuda.is_available() else -1\n",
    "    return pipeline(\n",
    "        \"translation\",\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        trust_remote_code=True,\n",
    "        device=device_idx\n",
    "    )\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_translation_pipe_indic_to_en():\n",
    "    tok = get_tokenizer_indic_en()\n",
    "    mdl = get_model_indic_en()\n",
    "    device_idx = 0 if torch.cuda.is_available() else -1\n",
    "    return pipeline(\n",
    "        \"translation\",\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        trust_remote_code=True,\n",
    "        device=device_idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a83d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Schemas\n",
    "# -----------------------------\n",
    "class TranslationRequest(BaseModel):\n",
    "    source_text: str\n",
    "    target_lang: str  # 'hi', 'ta', etc.\n",
    "    source_lang: str = \"en\"  # default English, but allow any supported\n",
    "\n",
    "# -----------------------------\n",
    "# Core translation\n",
    "# -----------------------------\n",
    "@lru_cache(maxsize=512)\n",
    "def cached_translation(source_text: str, target_lang: str, source_lang: str = \"en\") -> str:\n",
    "    if target_lang not in LANGUAGE_TAGS:\n",
    "        raise ValueError(f\"Unsupported target language: {target_lang}\")\n",
    "    if source_lang not in LANGUAGE_TAGS:\n",
    "        raise ValueError(f\"Unsupported source language: {source_lang}\")\n",
    "    if source_lang == target_lang:\n",
    "        return (source_text or \"\").strip()\n",
    "\n",
    "    tgt_tag = LANGUAGE_TAGS[target_lang]\n",
    "    src_tag = LANGUAGE_TAGS[source_lang]\n",
    "    tgt_iso, tgt_script = tgt_tag.split(\"_\")\n",
    "\n",
    "    text = (source_text or \"\").strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    def generate_with_tags(text: str, src: str, tgt: str, use_indic_en: bool = False) -> str:\n",
    "        if use_indic_en:\n",
    "            tok, mdl = get_tokenizer_indic_en(), get_model_indic_en()\n",
    "        else:\n",
    "            tok, mdl = get_tokenizer(), get_model()\n",
    "        # Format: \"{src_tag} {tgt_tag} {text}\"\n",
    "        tagged = f\"{src} {tgt} {text}\"\n",
    "        inputs = tok(tagged, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = mdl.generate(\n",
    "                **inputs,\n",
    "                max_length=512,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "                pad_token_id=tok.pad_token_id,\n",
    "                eos_token_id=tok.eos_token_id,\n",
    "            )\n",
    "        cand = tok.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        return cand\n",
    "\n",
    "    try:\n",
    "        if source_lang == \"en\":\n",
    "            cand = generate_with_tags(text, src_tag, tgt_tag, use_indic_en=False)\n",
    "        elif target_lang == \"en\":\n",
    "            cand = generate_with_tags(text, src_tag, \"eng_Latn\", use_indic_en=True)\n",
    "        else:\n",
    "            # Indic -> Indic via English pivot\n",
    "            mid = generate_with_tags(text, src_tag, \"eng_Latn\", use_indic_en=True)\n",
    "            cand = generate_with_tags(mid, \"eng_Latn\", tgt_tag, use_indic_en=False)\n",
    "\n",
    "        # Script correction for non-English targets\n",
    "        if target_lang != \"en\" and not looks_like_script(cand, tgt_script) and looks_like_script(cand, \"Deva\"):\n",
    "            cand = transliterate_if_needed(cand, tgt_script)\n",
    "        return cand\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Model translation failed: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ddd08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Routes\n",
    "# -----------------------------\n",
    "app = app  # keep reference name stable\n",
    "\n",
    "@app.post(\"/api/v1/translate\")\n",
    "def translate(request: TranslationRequest):\n",
    "    try:\n",
    "        translated_text = cached_translation(\n",
    "            request.source_text,\n",
    "            request.target_lang,\n",
    "            request.source_lang,\n",
    "        )\n",
    "        return {\"translated_text\": translated_text}\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))\n",
    "    except RuntimeError as re:\n",
    "        raise HTTPException(status_code=500, detail=str(re))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d612f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fa06910",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    try:\n",
    "        _tok = get_tokenizer()\n",
    "        _mdl = get_model()\n",
    "        _pipe_en_indic = get_translation_pipe_en_to_indic()\n",
    "        _pipe_indic_en = get_translation_pipe_indic_to_en()\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"device\": str(DEVICE),\n",
    "            \"model_loaded\": _mdl is not None,\n",
    "            \"tokenizer_loaded\": _tok is not None,\n",
    "            \"pipeline_en_indic\": _pipe_en_indic is not None,\n",
    "            \"pipeline_indic_en\": _pipe_indic_en is not None,\n",
    "            \"translit_enabled\": bool(itransliterate and SANSCRIPT_MAP),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(e),\n",
    "            \"device\": str(DEVICE),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5b24e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [88268]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up... Loading model/tokenizer/pipelines\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded\n",
      "Loading model...\n",
      "Trying model strategy 1...\n",
      "Tokenizer loaded\n",
      "Loading model...\n",
      "Trying model strategy 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "Loading INDIC→EN tokenizer...\n",
      "INDIC→EN tokenizer loaded\n",
      "Loading INDIC→EN model...\n",
      "Trying INDIC→EN model strategy 1...\n",
      "INDIC→EN tokenizer loaded\n",
      "Loading INDIC→EN model...\n",
      "Trying INDIC→EN model strategy 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIC→EN model loaded on cpu\n",
      "Resources loaded successfully\n",
      "INFO:     127.0.0.1:55994 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55994 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:56145 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:56145 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57183 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57183 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57212 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57212 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [88268]\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [88268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Entrypoint (Azure PORT-ready)\n",
    "# -----------------------------\n",
    "try:\n",
    "    get_ipython  # defined only in IPython/Jupyter\n",
    "    IN_IPYTHON = True\n",
    "except NameError:\n",
    "    IN_IPYTHON = False\n",
    "\n",
    "if IN_IPYTHON:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    port = int(os.environ.get(\"PORT\", 8000))\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()\n",
    "else:\n",
    "    if __name__ == \"__main__\":\n",
    "        port = int(os.environ.get(\"PORT\", 8000))\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=port)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BITSAIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
