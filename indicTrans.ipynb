{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b8885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import uvicorn\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad7a2a",
   "metadata": {},
   "source": [
    "#### Optional transliteration fallback (no indictrans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14885e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from indic_transliteration import sanscript\n",
    "    from indic_transliteration.sanscript import transliterate as itransliterate\n",
    "except Exception:\n",
    "    sanscript = None\n",
    "    itransliterate = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4045ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_ID = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "INDIC_EN_MODEL_ID = \"ai4bharat/indictrans2-indic-en-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7e0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_TAGS = {\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"hi\": \"hin_Deva\",\n",
    "    \"ta\": \"tam_Taml\",\n",
    "    \"te\": \"tel_Telu\",\n",
    "    \"kn\": \"kan_Knda\",\n",
    "    \"ml\": \"mal_Mlym\",\n",
    "    \"bn\": \"ben_Beng\",\n",
    "    \"mr\": \"mar_Deva\",\n",
    "    \"gu\": \"guj_Gujr\",\n",
    "    \"or\": \"ory_Orya\",  # if this fails for your model snapshot, try \"ori_Orya\"\n",
    "    \"pa\": \"pan_Guru\"\n",
    "}\n",
    "\n",
    "LANGUAGE_ISO3 = {k: v.split(\"_\")[0] for k, v in LANGUAGE_TAGS.items()}\n",
    "LANGUAGE_SCRIPT = {k: v.split(\"_\")[1] for k, v in LANGUAGE_TAGS.items()}\n",
    "\n",
    "# Unicode script ranges for sanity check\n",
    "SCRIPT_RANGES = {\n",
    "    \"Latn\": (0x0041, 0x007A),  # coarse Latin range (A-z)\n",
    "    \"Deva\": (0x0900, 0x097F),\n",
    "    \"Beng\": (0x0980, 0x09FF),\n",
    "    \"Guru\": (0x0A00, 0x0A7F),\n",
    "    \"Gujr\": (0x0A80, 0x0AFF),\n",
    "    \"Orya\": (0x0B00, 0x0B7F),\n",
    "    \"Taml\": (0x0B80, 0x0BFF),\n",
    "    \"Telu\": (0x0C00, 0x0C7F),\n",
    "    \"Knda\": (0x0C80, 0x0CFF),\n",
    "    \"Mlym\": (0x0D00, 0x0D7F),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30437a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_script(s: str, script: str) -> bool:\n",
    "    lo, hi = SCRIPT_RANGES.get(script, (None, None))\n",
    "    if lo is None:  # unknown script key → don't block\n",
    "        return True\n",
    "    return any(lo <= ord(ch) <= hi for ch in s)\n",
    "\n",
    "# Map our script keys -> indic-transliteration constants\n",
    "SANSCRIPT_MAP = None\n",
    "if sanscript is not None:\n",
    "    SANSCRIPT_MAP = {\n",
    "        \"Deva\": getattr(sanscript, \"DEVANAGARI\", None),\n",
    "        \"Beng\": getattr(sanscript, \"BENGALI\", None),\n",
    "        \"Guru\": getattr(sanscript, \"GURMUKHI\", None),\n",
    "        \"Gujr\": getattr(sanscript, \"GUJARATI\", None),\n",
    "        \"Orya\": getattr(sanscript, \"ORIYA\", None),   # a.k.a. Odia\n",
    "        \"Taml\": getattr(sanscript, \"TAMIL\", None),\n",
    "        \"Telu\": getattr(sanscript, \"TELUGU\", None),\n",
    "        \"Knda\": getattr(sanscript, \"KANNADA\", None),\n",
    "        \"Mlym\": getattr(sanscript, \"MALAYALAM\", None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_if_needed(text: str, target_script: str) -> str:\n",
    "    \"\"\"\n",
    "    If output isn't in target script but is Devanagari, try converting\n",
    "    Devanagari -> target_script using indic-transliteration (if available).\n",
    "    \"\"\"\n",
    "    if looks_like_script(text, target_script):\n",
    "        return text\n",
    "    if looks_like_script(text, \"Deva\") and itransliterate and SANSCRIPT_MAP:\n",
    "        src = SANSCRIPT_MAP.get(\"Deva\")\n",
    "        dst = SANSCRIPT_MAP.get(target_script)\n",
    "        if src and dst:\n",
    "            try:\n",
    "                return itransliterate(text, src, dst)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81702eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Romanized Indic fallback: Latin -> target script via indic-transliteration\n",
    "\n",
    "def roman_to_script(text: str, target_script: str) -> str:\n",
    "    if not (itransliterate and SANSCRIPT_MAP):\n",
    "        return text\n",
    "    dst = SANSCRIPT_MAP.get(target_script)\n",
    "    if not dst:\n",
    "        return text\n",
    "    # Try common Roman schemes\n",
    "    for scheme_name in (\"ITRANS\", \"HK\", \"IAST\"):\n",
    "        src = getattr(sanscript, scheme_name, None)\n",
    "        if src is None:\n",
    "            continue\n",
    "        try:\n",
    "            out = itransliterate(text, src, dst)\n",
    "            if looks_like_script(out, target_script):\n",
    "                return out\n",
    "        except Exception:\n",
    "            continue\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_ascii_roman(s: str) -> bool:\n",
    "    return all(ord(c) < 128 for c in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b375e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# App lifecycle\n",
    "# -----------------------------\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    try:\n",
    "        print(\"Starting up... Loading model/tokenizer/pipelines\")\n",
    "        get_tokenizer()\n",
    "        get_model()\n",
    "        get_translation_pipe_en_to_indic()\n",
    "        get_translation_pipe_indic_to_en()\n",
    "        # Also ensure explicit-generate models are ready\n",
    "        get_tokenizer_indic_en()\n",
    "        get_model_indic_en()\n",
    "        print(\"Resources loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not pre-load resources: {e}\")\n",
    "    yield\n",
    "    print(\"Shutting down...\")\n",
    "\n",
    "app = FastAPI(lifespan=lifespan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09c2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static & templates\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "templates = Jinja2Templates(directory=\"templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee1bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORS (relax in dev)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5df3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cached loaders\n",
    "# -----------------------------\n",
    "@lru_cache(maxsize=1)\n",
    "def load_tokenizer():\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None\n",
    "    )\n",
    "    print(\"Tokenizer loaded\")\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2218fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "    configs = [\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None,\n",
    "             low_cpu_mem_usage=True),\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None),\n",
    "        dict(trust_remote_code=True),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for i, cfg in enumerate(configs, 1):\n",
    "        try:\n",
    "            print(f\"Trying model strategy {i}...\")\n",
    "            mdl = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, **cfg)\n",
    "\n",
    "            # Disable caching to avoid \"Cache only has 0 layers\"\n",
    "            for attr in (\"config\", \"generation_config\"):\n",
    "                obj = getattr(mdl, attr, None)\n",
    "                if obj is not None:\n",
    "                    try: setattr(obj, \"use_cache\", False)\n",
    "                    except Exception: pass\n",
    "            try: mdl.config.cache_implementation = None\n",
    "            except Exception: pass\n",
    "            try: mdl.cache_implementation = None\n",
    "            except Exception: pass\n",
    "\n",
    "            mdl.eval().to(DEVICE)\n",
    "            print(f\"Model loaded on {DEVICE}\")\n",
    "            return mdl\n",
    "        except Exception as e:\n",
    "            print(f\"Strategy {i} failed: {e}\")\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed to load model: {last_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb74beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def get_tokenizer():\n",
    "    global tokenizer\n",
    "    if tokenizer is None:\n",
    "        tokenizer = load_tokenizer()\n",
    "    return tokenizer\n",
    "\n",
    "def get_model():\n",
    "    global model\n",
    "    if model is None:\n",
    "        model = load_model()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2234f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional loaders for Indic→English\n",
    "@lru_cache(maxsize=1)\n",
    "def load_tokenizer_indic_en():\n",
    "    print(\"Loading INDIC→EN tokenizer...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        INDIC_EN_MODEL_ID,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None\n",
    "    )\n",
    "    print(\"INDIC→EN tokenizer loaded\")\n",
    "    return tok\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_model_indic_en():\n",
    "    print(\"Loading INDIC→EN model...\")\n",
    "    configs = [\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None,\n",
    "             low_cpu_mem_usage=True),\n",
    "        dict(trust_remote_code=True,\n",
    "             torch_dtype=torch.float32,\n",
    "             cache_dir=\"/app/.cache\" if os.path.exists(\"/app\") else None),\n",
    "        dict(trust_remote_code=True),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for i, cfg in enumerate(configs, 1):\n",
    "        try:\n",
    "            print(f\"Trying INDIC→EN model strategy {i}...\")\n",
    "            mdl = AutoModelForSeq2SeqLM.from_pretrained(INDIC_EN_MODEL_ID, **cfg)\n",
    "            for attr in (\"config\", \"generation_config\"):\n",
    "                obj = getattr(mdl, attr, None)\n",
    "                if obj is not None:\n",
    "                    try: setattr(obj, \"use_cache\", False)\n",
    "                    except Exception: pass\n",
    "            try: mdl.config.cache_implementation = None\n",
    "            except Exception: pass\n",
    "            try: mdl.cache_implementation = None\n",
    "            except Exception: pass\n",
    "            mdl.eval().to(DEVICE)\n",
    "            print(f\"INDIC→EN model loaded on {DEVICE}\")\n",
    "            return mdl\n",
    "        except Exception as e:\n",
    "            print(f\"INDIC→EN strategy {i} failed: {e}\")\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed to load INDIC→EN model: {last_err}\")\n",
    "\n",
    "# Getters\n",
    "@lru_cache(maxsize=1)\n",
    "def get_tokenizer_indic_en():\n",
    "    return load_tokenizer_indic_en()\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_model_indic_en():\n",
    "    return load_model_indic_en()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd65bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def get_translation_pipe_en_to_indic():\n",
    "    tok = get_tokenizer()\n",
    "    mdl = get_model()\n",
    "    device_idx = 0 if torch.cuda.is_available() else -1\n",
    "    return pipeline(\n",
    "        \"translation\",\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        trust_remote_code=True,\n",
    "        device=device_idx\n",
    "    )\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_translation_pipe_indic_to_en():\n",
    "    tok = get_tokenizer_indic_en()\n",
    "    mdl = get_model_indic_en()\n",
    "    device_idx = 0 if torch.cuda.is_available() else -1\n",
    "    return pipeline(\n",
    "        \"translation\",\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        trust_remote_code=True,\n",
    "        device=device_idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a83d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Schemas\n",
    "# -----------------------------\n",
    "class TranslationRequest(BaseModel):\n",
    "    source_text: str\n",
    "    target_lang: str  # 'hi', 'ta', etc.\n",
    "    source_lang: str = \"en\"  # default English, but allow any supported\n",
    "\n",
    "# -----------------------------\n",
    "# Core translation (explicit generate with tags + fallbacks)\n",
    "# -----------------------------\n",
    "@lru_cache(maxsize=512)\n",
    "def cached_translation(source_text: str, target_lang: str, source_lang: str = \"en\") -> str:\n",
    "    if target_lang not in LANGUAGE_TAGS:\n",
    "        raise ValueError(f\"Unsupported target language: {target_lang}\")\n",
    "    if source_lang not in LANGUAGE_TAGS:\n",
    "        raise ValueError(f\"Unsupported source language: {source_lang}\")\n",
    "    if source_lang == target_lang:\n",
    "        return (source_text or \"\").strip()\n",
    "\n",
    "    tgt_tag = LANGUAGE_TAGS[target_lang]\n",
    "    src_tag = LANGUAGE_TAGS[source_lang]\n",
    "    tgt_iso, tgt_script = tgt_tag.split(\"_\")\n",
    "\n",
    "    text = (source_text or \"\").strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    def generate_with_tags(text: str, src: str, tgt: str, use_indic_en: bool = False) -> str:\n",
    "        if use_indic_en:\n",
    "            tok, mdl = get_tokenizer_indic_en(), get_model_indic_en()\n",
    "        else:\n",
    "            tok, mdl = get_tokenizer(), get_model()\n",
    "        tagged = f\"{src} {tgt} {text}\"\n",
    "        inputs = tok(tagged, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = mdl.generate(\n",
    "                **inputs,\n",
    "                max_length=512,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "                pad_token_id=tok.pad_token_id,\n",
    "                eos_token_id=tok.eos_token_id,\n",
    "            )\n",
    "        cand = tok.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        return cand\n",
    "\n",
    "    try:\n",
    "        if source_lang == \"en\":\n",
    "            cand = generate_with_tags(text, src_tag, tgt_tag, use_indic_en=False)\n",
    "        elif target_lang == \"en\":\n",
    "            cand = generate_with_tags(text, src_tag, \"eng_Latn\", use_indic_en=True)\n",
    "        else:\n",
    "            mid = generate_with_tags(text, src_tag, \"eng_Latn\", use_indic_en=True)\n",
    "            cand = generate_with_tags(mid, \"eng_Latn\", tgt_tag, use_indic_en=False)\n",
    "\n",
    "        if target_lang != \"en\":\n",
    "            if not looks_like_script(cand, tgt_script) and looks_like_script(cand, \"Deva\"):\n",
    "                cand = transliterate_if_needed(cand, tgt_script)\n",
    "            elif source_lang == \"en\" and is_ascii_roman(text):\n",
    "                rom = roman_to_script(text, tgt_script)\n",
    "                if looks_like_script(rom, tgt_script):\n",
    "                    return rom\n",
    "        return cand\n",
    "    except Exception:\n",
    "        # Fallbacks for untranslatable content\n",
    "        if target_lang != \"en\" and source_lang == \"en\" and is_ascii_roman(text):\n",
    "            rom = roman_to_script(text, tgt_script)\n",
    "            if looks_like_script(rom, tgt_script):\n",
    "                return rom\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ddd08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Routes\n",
    "# -----------------------------\n",
    "app = app  # keep reference name stable\n",
    "\n",
    "@app.post(\"/api/v1/translate\")\n",
    "def translate(request: TranslationRequest):\n",
    "    try:\n",
    "        translated_text = cached_translation(\n",
    "            request.source_text,\n",
    "            request.target_lang,\n",
    "            request.source_lang,\n",
    "        )\n",
    "        return {\"translated_text\": translated_text}\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))\n",
    "    except RuntimeError as re:\n",
    "        raise HTTPException(status_code=500, detail=str(re))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d612f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fa06910",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    try:\n",
    "        _tok = get_tokenizer()\n",
    "        _mdl = get_model()\n",
    "        _pipe_en_indic = get_translation_pipe_en_to_indic()\n",
    "        _pipe_indic_en = get_translation_pipe_indic_to_en()\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"device\": str(DEVICE),\n",
    "            \"model_loaded\": _mdl is not None,\n",
    "            \"tokenizer_loaded\": _tok is not None,\n",
    "            \"pipeline_en_indic\": _pipe_en_indic is not None,\n",
    "            \"pipeline_indic_en\": _pipe_indic_en is not None,\n",
    "            \"translit_enabled\": bool(itransliterate and SANSCRIPT_MAP),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(e),\n",
    "            \"device\": str(DEVICE),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af4620",
   "metadata": {},
   "source": [
    "## Evaluation: BLEU, METEOR, TER and Google Translate Comparison\n",
    "\n",
    "This section computes BLEU, METEOR, and TER scores for the in-notebook translator and compares outputs against Google Translate.\n",
    "- Requires small eval pairs (src, ref) per language.\n",
    "- Uses sacrebleu for BLEU/TER; NLTK for METEOR (limited for non-English); also reports ChrF as an extra robust metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f577c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sacrebleu nltk googletrans==4.0.0-rc1 --quiet\n",
    "import asyncio\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "import sacrebleu\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "try:\n",
    "    from googletrans import Translator as GoogleTranslator\n",
    "except Exception:\n",
    "    GoogleTranslator = None\n",
    "\n",
    "# Ensure NLTK resources for METEOR tokenization\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de84c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(pairs: List[Tuple[str, str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    pairs: list of (source_text, source_lang, target_lang)\n",
    "    returns: list of hypothesis strings\n",
    "    \"\"\"\n",
    "    hyps = []\n",
    "    for text, sl, tl in pairs:\n",
    "        try:\n",
    "            hyps.append(cached_translation(text, target_lang=tl, source_lang=sl))\n",
    "        except Exception as e:\n",
    "            hyps.append(\"\")\n",
    "    return hyps\n",
    "\n",
    "\n",
    "def compute_metrics(refs: List[str], hyps: List[str]):\n",
    "    bleu = BLEU(tokenize=\"intl\")\n",
    "    chrf = CHRF(word_order=2)\n",
    "    ter = TER()\n",
    "\n",
    "    # sacrebleu expects list of references (possibly multi-ref)\n",
    "    bleu_score = bleu.corpus_score(hyps, [refs])\n",
    "    chrf_score = chrf.corpus_score(hyps, [refs])\n",
    "    ter_score = ter.corpus_score(hyps, [refs])\n",
    "\n",
    "    # METEOR: average sentence METEOR (primarily designed for English)\n",
    "    meteor_vals = []\n",
    "    for h, r in zip(hyps, refs):\n",
    "        try:\n",
    "            meteor_vals.append(meteor_score([r], h))\n",
    "        except Exception:\n",
    "            meteor_vals.append(0.0)\n",
    "    meteor_avg = sum(meteor_vals) / max(1, len(meteor_vals))\n",
    "\n",
    "    return dict(\n",
    "        BLEU=bleu_score.score,\n",
    "        ChrF=chrf_score.score,\n",
    "        TER=ter_score.score,\n",
    "        METEOR=meteor_avg * 100.0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33e66b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizer loaded\n",
      "Loading model...\n",
      "Trying model strategy 1...\n",
      "Tokenizer loaded\n",
      "Loading model...\n",
      "Trying model strategy 1...\n",
      "Model loaded on cpu\n",
      "Model loaded on cpu\n",
      "Loading INDIC→EN tokenizer...\n",
      "Loading INDIC→EN tokenizer...\n",
      "INDIC→EN tokenizer loaded\n",
      "Loading INDIC→EN model...\n",
      "Trying INDIC→EN model strategy 1...\n",
      "INDIC→EN tokenizer loaded\n",
      "Loading INDIC→EN model...\n",
      "Trying INDIC→EN model strategy 1...\n",
      "INDIC→EN model loaded on cpu\n",
      "INDIC→EN model loaded on cpu\n",
      "Our en→hi: {\n",
      "  \"BLEU\": 5.035625662261279,\n",
      "  \"ChrF\": 6.299278694954009,\n",
      "  \"TER\": 110.00000000000001,\n",
      "  \"METEOR\": 0.0\n",
      "}\n",
      "Our hi→en: {\n",
      "  \"BLEU\": 42.323094193118685,\n",
      "  \"ChrF\": 63.904565631107445,\n",
      "  \"TER\": 45.45454545454545,\n",
      "  \"METEOR\": 0.0\n",
      "}\n",
      "googletrans not installed; skipping Google comparison.\n",
      "Our en→hi: {\n",
      "  \"BLEU\": 5.035625662261279,\n",
      "  \"ChrF\": 6.299278694954009,\n",
      "  \"TER\": 110.00000000000001,\n",
      "  \"METEOR\": 0.0\n",
      "}\n",
      "Our hi→en: {\n",
      "  \"BLEU\": 42.323094193118685,\n",
      "  \"ChrF\": 63.904565631107445,\n",
      "  \"TER\": 45.45454545454545,\n",
      "  \"METEOR\": 0.0\n",
      "}\n",
      "googletrans not installed; skipping Google comparison.\n"
     ]
    }
   ],
   "source": [
    "# Example evaluation data (tiny set for demo)\n",
    "# Each item: (source_text, source_lang, target_lang, reference)\n",
    "en_hi_data = [\n",
    "    (\"Hello, how are you?\", \"en\", \"hi\", \"नमस्ते, आप कैसे हैं?\"),\n",
    "    (\"Good morning everyone.\", \"en\", \"hi\", \"सुप्रभात सभी को।\"),\n",
    "    (\"Where is the market?\", \"en\", \"hi\", \"बाज़ार कहाँ है?\"),\n",
    "]\n",
    "\n",
    "hi_en_data = [\n",
    "    (\"नमस्ते, आप कैसे हैं?\", \"hi\", \"en\", \"Hello, how are you?\"),\n",
    "    (\"सुप्रभात सभी को।\", \"hi\", \"en\", \"Good morning everyone.\"),\n",
    "    (\"बाज़ार कहाँ है?\", \"hi\", \"en\", \"Where is the market?\"),\n",
    "]\n",
    "\n",
    "\n",
    "def run_eval(eval_data):\n",
    "    pairs = [(s, sl, tl) for s, sl, tl, _ in eval_data]\n",
    "    refs = [r for _, _, _, r in eval_data]\n",
    "    hyps = translate_batch(pairs)\n",
    "    scores = compute_metrics(refs, hyps)\n",
    "    return hyps, scores\n",
    "\n",
    "# Run our model evals\n",
    "our_en_hi_hyps, our_en_hi_scores = run_eval(en_hi_data)\n",
    "our_hi_en_hyps, our_hi_en_scores = run_eval(hi_en_data)\n",
    "print(\"Our en→hi:\", json.dumps(our_en_hi_scores, indent=2, ensure_ascii=False))\n",
    "print(\"Our hi→en:\", json.dumps(our_hi_en_scores, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Google Translate baselines (if available)\n",
    "if GoogleTranslator is not None:\n",
    "    gt = GoogleTranslator()\n",
    "    def gt_translate_batch(pairs):\n",
    "        outs = []\n",
    "        for text, sl, tl in pairs:\n",
    "            try:\n",
    "                # googletrans uses language codes like 'en', 'hi'\n",
    "                outs.append(gt.translate(text, src=sl, dest=tl).text)\n",
    "            except Exception:\n",
    "                outs.append(\"\")\n",
    "        return outs\n",
    "\n",
    "    gt_en_hi_hyps = gt_translate_batch([(s, sl, tl) for s, sl, tl, _ in en_hi_data])\n",
    "    gt_hi_en_hyps = gt_translate_batch([(s, sl, tl) for s, sl, tl, _ in hi_en_data])\n",
    "\n",
    "    gt_en_hi_scores = compute_metrics([r for _, _, _, r in en_hi_data], gt_en_hi_hyps)\n",
    "    gt_hi_en_scores = compute_metrics([r for _, _, _, r in hi_en_data], gt_hi_en_hyps)\n",
    "\n",
    "    print(\"Google en→hi:\", json.dumps(gt_en_hi_scores, indent=2, ensure_ascii=False))\n",
    "    print(\"Google hi→en:\", json.dumps(gt_hi_en_scores, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"googletrans not installed; skipping Google comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b24e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [92123]\n",
      "INFO:     Waiting for application startup.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Waiting for application startup.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up... Loading model/tokenizer/pipelines\n",
      "Resources loaded successfully\n",
      "INFO:     127.0.0.1:52797 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52797 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52798 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52798 - \"POST /api/v1/translate HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Entrypoint (Azure PORT-ready)\n",
    "# -----------------------------\n",
    "try:\n",
    "    get_ipython  # defined only in IPython/Jupyter\n",
    "    IN_IPYTHON = True\n",
    "except NameError:\n",
    "    IN_IPYTHON = False\n",
    "\n",
    "if IN_IPYTHON:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    port = int(os.environ.get(\"PORT\", 8000))\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()\n",
    "else:\n",
    "    if __name__ == \"__main__\":\n",
    "        port = int(os.environ.get(\"PORT\", 8000))\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=port)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BITSAIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
